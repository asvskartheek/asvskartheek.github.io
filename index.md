---
layout: default
title: Kartheek Akella
---


## Research Interests
- Abstraction and Generalization
- Over-parameterization
- Bayesian Deep Learning
- Machine Translation

## Bio

I am a research fellow at [IIIT Hyderabad](https://iiit.ac.in/), where I am jointly advised by  [Prof. C. V. Jawahar](http://faculty.iiit.ac.in/~jawahar/) and [Prof. Vinay P. Namboodiri](https://www.cse.iitk.ac.in/users/vinaypn/) at [Center for Visual Information Technology](http://cvit.iiit.ac.in/). My research is focused on neural machine translation for low resource languages using low compute resources.

Prior to this, I have spent a brief time as a research assistant at [Institute of Infocomm Research (I2R)](https://www.a-star.edu.sg/i2r), A*STAR in Singapore working on the problem of dynamic analysis of benign programs, under the supervision of [Dr. Sai Praveen Kadiyala](https://www.linkedin.com/in/kadiyala-sai-praveen-92131733/). 

<!--
Dummy Paragraph
-->

I graduated from [BITS Pilani](http://iitism.ac.in), India, in 2020 with a B.E in Computer Science. During my undergraduate years I worked closely with [Prof. Bhanumurthy](https://www.bits-pilani.ac.in/hyderabad/bhanumurthy/Profile) and [Prof. Aruna Malapati](https://universe.bits-pilani.ac.in/hyderabad/arunamalapati/Profile) on projects related to Machine Learning and Natural Language Processing for Telugu. I also worked with [Prof. G Geethakumari](https://scholar.google.com/citations?user=IKddJZEAAAAJ&hl=en) on detecting RPL Attacks in Low Power and Lossy Networks (LLN).

***

## Updates

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;
-----|-----
**\[1 Dec 2020\]** | Our Work done during at CVIT, IIIT-H got accepted at [ICON](https://www.iitp.ac.in/~ai-nlp-ml/icon2020/main_prog.html) 2020.
**\[18 Nov 2020\]** | Our Work done during my thesis at I2R, A*STAR got accepted at DYNAMICS Workshop part of [ACSAC](https://www.acsac.org/) 2020.
**\[10 Nov 2020\]** | Started my [YouTube channel](https://www.youtube.com/channel/UCV7-XAVPAH4c7Xe7EwNTaAw) on explaining ML and Deep Learning papers -- Inspired by [Yannic Kilcher](https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfew).
**\[6 Nov 2020\]** | Our Work done during my thesis at I2R, A*STAR has been filed for an internal patent.
**\[18 Sep 2020\]** | Participated in [Indic Multilingual Task](http://lotus.kuee.kyoto-u.ac.jp/WAT/indic-multilingual/index.html) in of Workshop on Asian Translation 2020 as team **cvit**. Produced close to SOTA results in four languages -- [Odiya](http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/list.php?t=89&o=7) (2nd), [Malayalam](http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/list.php?t=119&o=7) (2nd), [Telugu](http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/list.php?t=125&o=7) (3rd) and [Tamil](http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/list.php?t=123&o=7) (3rd)
**\[1 Aug 2020\]** | Joined Center for Visual Information Technology (CVIT), IIIT Hyderabad as a research fellow under Prof. C V Jawahar.
**\[8 Jul 2020\]** | Submitted my undergraduate thesis at BITS Hyderabad.
**\[17 Feb 2020\]** | Joined Institute of Infocomm Research (I2R), A*STAR in Singapore as a research assistant under Dr. Sai Praveen Kadiyala.
**\[12 Dec 2019\]** | Accepted position as RA at Institute of Infocomm Research (I2R), A*STAR in Singapore.
**\[20 May 2019\]** | Joined as a "TA" for [Deep Learning at WILP, BITS](https://bits-pilani-wilp.ac.in/m-tech/cluster/data-science-and-engineering.php).
**\[21 May 2018\]** | Joined National Textile Corporation, Coimbatore as a Software Developer Intern.


***

## Publications

&nbsp;
1. **Kartheek Akella**, Sai Himal Allu, Sridhar Suresh Ragupathi, Aman Singhal, Zeeshan Khan, C.V. Jawahar and Vinay P. Namboodiri
. Exploring Pair-Wise NMT for Indian Languages. *Accepted at ICON (International Conference on Natural Language Processing) 2020, Patna, India*

&nbsp;
2. Sai Praveen Kadiyala, **Kartheek Akella**, Truong Huu Tram. Program Behavior Analysis and Clustering using Performance Counters. *Accepted at DYNAMICS (DYnamic and Novel Advances in Machine Learning and Intelligent Cyber Security) part of ACSAC 2020, Austin, Texas*

**1 Papers under peer review**


<!---
1. Abhishek Jha, Vikram Voleti, Vinay Namboodiri and C.V. Jawahar, ”Lip-Synchronization for Dubbed Instructional Videos”, Fine-grained Instructional Video Understanding, CVPR Workshop 2018. \[[Paper](http://fiver.eecs.umich.edu/abstracts/CVPRW_2018_FIVER_A_Jha.pdf)\] \[[Poster](https://drive.google.com/file/d/19eTyXoDtKo_txxRRylg0mM9oQS7iKVaQ/view?usp=sharing)\]

1. Sai Praveen Kadiyala, **Kartheek Akella**, Truong Huu Tram. Program Behavior Analysis and Clustering using Performance Counters. (Under Peer Review)

2. Sai Himal Allu, **Kartheek Akella**, Sridhar Raghuram, Aman Singhal, Zeeshan Khan, Vinay Namboodiri, CV Jawahar. 

2. Sahil Chelaramani, Abhishek Jha and Anoop Namboodiri, ”Cross-modal style transfer”, 25th IEEE International Conference on Image Processing (ICIP) 2018. \[[Paper](https://drive.google.com/file/d/1lT8HNGrsUio9MW87XNattaUz5hsBkXYO/view?usp=sharing)\]


3. Abhishek  Jha,  Vinay  Namboodiri  and  C.V.  Jawahar,  ”Word  Spotting  in  Silent  Lip  Videos”,  IEEE  Winter
Conference on Applications of Computer Vision (WACV) 2018. \[[Paper](https://cvit.iiit.ac.in/images/ConferencePapers/2018/Word-Spotting-in-Silent-Lip-Videos.pdf)\] \[[Poster](https://drive.google.com/file/d/1XAE6gRhy2terH2DOmg87uEzpXzSDZgTk/view?usp=sharing)\] \[[Project page](https://cvit.iiit.ac.in/research/projects/cvit-projects/lip-word-spotting)\]


4. Yashaswi Verma, Abhishek Jha, C.V. Jawahar, ”Cross-specificity:  modelling data semantics for cross-modal
matching and retrieval”, International Journal of Multimedia Information Retrieval, Springer, June 2018. \[[Link](https://link.springer.com/article/10.1007/s13735-017-0138-7)\]
-->


<!--
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" style="border-style: none ">



<tbody>
	<tr>
		<td width="35%"><img src="/images/pic_icasssp_2019.png" alt="cross_lip_sync" width="250" style="border-style: none"></td>
		<td width="65%" valign="top">
			<p>
				<a href="https://drive.google.com/file/d/1a88OiddZDcfrE1yh8UORnMXLWe_YOcHY/view?usp=sharing">Cross-Language Speech Dependent Lip-Synchronization</a> <br>
				<strong>Abhishek Jha</strong>,
                <a href="https://voletiv.github.io/">Vikram Voleti</a>,
				<a href="https://www.cse.iitk.ac.in/users/vinaypn/">Vinay P. Namboodiri</a>,
				<a href="http://faculty.iiit.ac.in/~jawahar/">C. V. Jawahar</a> <br>
			<span style="color:#9A2617;">International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2019</span>
				<br>
				<a href="https://drive.google.com/file/d/1a88OiddZDcfrE1yh8UORnMXLWe_YOcHY/view?usp=sharing">[Link]</a>
				<br><br>
<div style="height:80px;width:500px;overflow:auto;background-color:#def;scrollbar-base-color:gold;font-family:sans-serif;font-size:10px;padding:10px;overflow:auto;border:1px solid #abf;"><strong>Abstract</strong><br>
Understanding videos of people speaking across international borders is hard as audiences from different demographies do not understand the language. Such speech videos are often supplemented with language subtitles. However, these hamper the viewing experience as the attention is shared. Simple audio dubbing in a different language makes the video appear unnatural due to unsynchronized lip motion. In this paper, we propose a system for automated cross-language lip synchronization for re-dubbed videos. Our model generates superior photorealistic lip-synchronization over original video in comparison to the current re-dubbing method. With the help of a user-based study, we verify that our method is preferred over unsynchronized videos.

</div>
			</p>
		</td>
		
	</tr>		
</tbody>








<tbody>
        <tr>
                <td width="35%"><img src="/images/mva_2018.png" alt="lip_word_spot" width="250" style="border-style: none"></td>
                <td width="65%" valign="top">
                        <p>
                                <a href="https://drive.google.com/file/d/1PXF46jspuTgMcXnyWcTgA90ABFI0LfyU/view?usp=sharing">
				Spotting Words in Silent Speech Videos : A Retrieval based approach</a>
                                <strong>Abhishek Jha</strong>,
                                <a href="https://www.cse.iitk.ac.in/users/vinaypn/">Vinay P. Namboodiri</a>,
                                <a href="http://faculty.iiit.ac.in/~jawahar/">C. V. Jawahar</a> <br>
                        <span style="color:#9A2617;">Journal of Machine Vision and Applications <strong>(MVA)</strong>, Springer, 2018</span>
                                <br><br>
                                <a href="https://drive.google.com/file/d/1PXF46jspuTgMcXnyWcTgA90ABFI0LfyU/view?usp=sharing">[Paper]</a>
                        </p>
                </td>

        </tr>
</tbody>


<tbody>
	<tr>
		<td width="35%"><img src="/images/cvpr_w_2018.png" alt="Visual_dub" width="250" style="border-style: none"></td>
		<td width="65%" valign="top">
			<p>
				<a href="http://fiver.eecs.umich.edu/abstracts/CVPRW_2018_FIVER_A_Jha.pdf">Lip-Synchronization for Dubbed Instructional Videos</a> <br>
				<strong>Abhishek Jha</strong>,
				Vikram Voleti,
				<a href="https://www.cse.iitk.ac.in/users/vinaypn/">Vinay P. Namboodiri</a>,
				<a href="http://faculty.iiit.ac.in/~jawahar/">C. V. Jawahar</a> <br>
			<span style="color:#9A2617;">FIVER, <strong>CVPR Workshop</strong> 2018</span>
				<br><br>
				<a href="http://fiver.eecs.umich.edu/abstracts/CVPRW_2018_FIVER_A_Jha.pdf">[Short Paper]</a> <a href="https://drive.google.com/file/d/19eTyXoDtKo_txxRRylg0mM9oQS7iKVaQ/view?usp=sharing">[Poster]</a>
			</p>
		</td>
		
	</tr>		
</tbody>



<tbody>
	<tr>
		<td width="35%"><img src="/images/icip_2018_3.png" alt="Visual_dub" width="250" style="border-style: none"></td>
		<td width="65%" valign="top">
			<p>
				<a href="https://drive.google.com/file/d/1lT8HNGrsUio9MW87XNattaUz5hsBkXYO/view?usp=sharing">Cross-modal style transfer</a> <br>
				Sahil Chelaramani,
				<strong>Abhishek Jha</strong>,
				<a href="https://faculty.iiit.ac.in/~anoop/">Anoop Namboodiri</a><br>
			<span style="color:#9A2617;">IEEE International Conference on Image Processing <strong>(ICIP)</strong> 2018</span>
				
				<br>
				<a href="https://drive.google.com/file/d/1lT8HNGrsUio9MW87XNattaUz5hsBkXYO/view?usp=sharing">[Paper]</a>
				<br><br>
				<div style="height:80px;width:500px;overflow:auto;background-color:#def;scrollbar-base-color:gold;font-family:sans-serif;font-size:10px;padding:10px;overflow:auto;border:1px solid #abf;">
				<strong>Abstract</strong><br>
				We, humans, have the ability to easily imagine scenes that
depict sentences such as “Today is a beautiful sunny day” or
“There is a Christmas feel, in the air”. While it is hard to

precisely describe what one person may imagine, the essen-
tial high-level themes associated with such sentences largely

remains the same. The ability to synthesize novel images that

depict the feel of a sentence is very useful in a variety of appli-
cations such as education, advertisement, and entertainment.

While existing papers tackle this problem given a style im-
age, we aim to provide a far more intuitive and easy to use

solution that synthesizes novel renditions of an existing im-
age, conditioned on a given sentence. We present a method

for cross-modal style transfer between an English sentence

and an image, to produce a new image that imbibes the essen-
tial theme of the sentence. We do this by modifying the style

transfer mechanism used in image style transfer to incorpo-
rate a style component derived from the given sentence. We

demonstrate promising results using the YFCC100m dataset.
</div>
			</p>
		</td>
		
	</tr>		
</tbody>







<tbody>
	<tr>
		<td width="35%"><img src="/images/wacv_2018.png" alt="Visual_dub" width="250" style="border-style: none"></td>
		<td width="65%" valign="top">
			<p>
				<a href="https://cvit.iiit.ac.in/images/ConferencePapers/2018/Word-Spotting-in-Silent-Lip-Videos.pdf">Word Spotting in Silent Lip Videos</a> <br>
				<strong>Abhishek Jha</strong>,
				<a href="https://www.cse.iitk.ac.in/users/vinaypn/">Vinay P. Namboodiri</a>,
				<a href="http://faculty.iiit.ac.in/~jawahar/">C. V. Jawahar</a> <br>
			<span style="color:#9A2617;">IEEE  Winter Conference on Applications of Computer Vision <strong>(WACV)</strong> 2018</span>
				<br>
				<a href="https://cvit.iiit.ac.in/images/ConferencePapers/2018/Word-Spotting-in-Silent-Lip-Videos.pdf">[Paper]</a> <a href="https://drive.google.com/file/d/1XAE6gRhy2terH2DOmg87uEzpXzSDZgTk/view?usp=sharing">[Poster]</a> <a href="https://cvit.iiit.ac.in/research/projects/cvit-projects/lip-word-spotting">[Project Page]</a>
				<br><br>

<div style="height:80px;width:500px;overflow:auto;background-color:#def;scrollbar-base-color:gold;font-family:sans-serif;font-size:10px;padding:10px;overflow:auto;border:1px solid #abf;">
				<strong>Abstract</strong><br>
				Our goal is to spot words in silent speech videos without
explicitly recognizing the spoken words, where the lip mo-
tion of the speaker is clearly visible and audio is absent. Ex-
isting work in this domain has mainly focused on recogniz-
ing a fixed set of words in word-segmented lip videos, which
limits the applicability of the learned model due to limited
vocabulary and high dependency on the model’s recogni-
tion performance.
Our contribution is two-fold:  1) we develop a pipeline
for  recognition-free  retrieval,  and  show  its  performance
against recognition-based retrieval on a large-scale dataset
and another set of out-of-vocabulary words.  2) We intro-
duce  a  query  expansion  technique  using  pseudo-relevant
feedback and propose a novel re-ranking method based on
maximizing the correlation between spatio-temporal land-
marks of the query and the top retrieval candidates.  Our
word  spotting  method  achieves  35%  higher  mean  aver-
age precision over recognition-based method on large-scale
LRW dataset. Finally, we demonstrate the application of the
method by word spotting in a popular speech video (“
The
great dictator
” by Charlie Chaplin) where we show that the
word retrieval can be used to understand what was spoken
perhaps in the silent movies.
	</div>
			</p>
		</td>
		
	</tr>		
</tbody>


<tbody>
	<tr>
		<td width="35%"><img src="/images/IJMIR_2018.png" alt="Visual_dub" width="250" style="border-style: none"></td>
		<td width="65%" valign="top">
			<p>
				<a href="https://link.springer.com/article/10.1007/s13735-017-0138-7">Cross-specificity:  modelling data semantics for cross-modal
matching and retrieval</a> <br>
				<a href="https://sites.google.com/view/yashaswiverma/">Yashaswi Verma</a>,
				<strong>Abhishek Jha</strong>,
				<a href="http://faculty.iiit.ac.in/~jawahar/">C. V. Jawahar</a> <br>
			<span style="color:#9A2617;">International Journal of Multimedia Information Retrieval, Springer, June 2018</span>
				<br>
				<a href="https://link.springer.com/article/10.1007/s13735-017-0138-7">[Link]</a>
				<br><br>
<div style="height:80px;width:500px;overflow:auto;background-color:#def;scrollbar-base-color:gold;font-family:sans-serif;font-size:10px;padding:10px;overflow:auto;border:1px solid #abf;"><strong>Abstract</strong><br>
While dealing with multi-modal data such as pairs of images and text, though individual samples may demonstrate inherent heterogeneity in their content, they are usually coupled with each other based on some higher-level concepts such as their categories. This shared information can be useful in measuring semantics of samples across modalities in a relative manner. In this paper, we investigate the problem of analysing the degree of specificity in the semantic content of a sample in one modality with respect to semantically similar samples in another modality. Samples that have high similarity with semantically similar samples from another modality are considered to be specific, while others are considered to be relatively ambiguous. To model this property, we propose a novel notion of “cross-specificity”. We present two mechanisms to measure cross-specificity: one based on human judgement and other based on an automated approach. We analyse different aspects of cross-specificity and demonstrate its utility in cross-modal retrieval task. Experiments show that though conceptually simple, it can benefit several existing cross-modal retrieval techniques and provide significant boost in their performance.

</div>
			</p>
		</td>
		
	</tr>		
</tbody>







</table>
-->


***

## Teaching

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;
-----|-----
August 2020: |  Teaching assistant (TA) in the course **Deep Learning**, WILP, BITS Pilani.
August 2019: | Teaching assistant (TA) in the course **Feature Engineering**, WILP, BITS Pilani. Course instructor: [Prof. Aruna Malapati](https://universe.bits-pilani.ac.in/hyderabad/arunamalapati/Profile)
May 2019: | Teaching assistant (TA) in the course **Regression**, WILP, BITS Pilani. Course instructor: [Prof. N L Bhanumurthy](https://www.bits-pilani.ac.in/hyderabad/bhanumurthy/Profile)



***

<!--
## Services

- *Reviewer*: 7th [National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics](http://ncvpripg.kletech.ac.in/) (**NCVPRIPG 2019**), December 22-24, 2019, Hublie.

- *Reviewer*: Second IAPR International Conference on [Computer Vision & Image Processing](https://www.iitr.ac.in/cvip2017/) (**CVIP-2017**, September 10-12, 2017), IIT Roorkee.

- *Organizing Team*:  17th [R&D showcase 2018](http://iiit.ac.in/randd/), IIIT Hyderabad: showcase of exhibits and demonstration research projects and represents of IIIT-H’s most recent developments in research and innovation in technology.

<iframe width="560" height="315" src="https://www.youtube.com/embed/kj_P-it-ATE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

\[[Telangana Today](https://telanganatoday.com/iiit-hyderabad-to-organise-rd-showcase-2018-from-feb-24)\] \[[APN News](https://www.apnnews.com/iiit-hyderabad-celebrates-17th-convocation/)\]

- *Organizing Team*: 1st [Computer Vision Summer School](http://cvit.iiit.ac.in/summerschoolseries/) 2016, IIIT Hyderabad.

<!--- - *Volunteer*: Technical Exhibition 2015, Jamshedpur, Tata Steel.

- *Web Developer & Proceedings*: IEEE International Conference on Microwave and Photonics (**ICMAP**) 2013, [IIT Dhanbad](https://www.iitism.ac.in/).
 -->


## Other Activity

- **[2017 - 2019]**: App Developer,  DoTA, Department of Technical Arts, BITS.

<!---
- **[2017 - Present]**: Student Admin, IIIT Hyderbad HPC cluster (aka ADA).

- **[2012 - 2013]**: Secretary, IEEE-Student branch, IIT(ISM) Dhanbad.

- **[2010 - 2012]**: Teacher, [Kartavya](http://www.kartavya.org/) , IIT(ISM) Dhanbad, an NGO for providing free and high quality education to underprivileged children living in slums and villages in india.

 -->















<!---
## Typography

# This is a [**link**](http://google.com). Something *italics* and something **bold**.

Here is a table

Year | Award | Category
-----|-------|--------
2014 | Emmy  | Won Outstanding Lead Actor in a miniseries or a movie
2015 | BAFTA | Nominated for Best Leading Actor for Sherlock
2014 | Satellite | Won Best Actor miniseries or television film

Here is a horizontal rule

---

Here is a blockquote

> To a great mind, nothing is little

## References

* Foo Bar: Head of Department, Placeholder Names, Lorem
* John Doe: Associate Professor, Department of Computer Science, Ipsum
-->
